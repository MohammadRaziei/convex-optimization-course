\documentclass[12pt,onecolumn,a4paper]{article}
\usepackage{epsfig,graphicx,subfigure,amsthm,amsmath,bm,dsfont}
\usepackage{color,xcolor}
\usepackage{float,placeins}

\usepackage{hyperref}

\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}

\usepackage{listings}

\usepackage{xepersian}
\settextfont[Scale=1.2]{XB Niloofar}
\setlatintextfont[Scale=1]{Times New Roman}
\setmathdigitfont{Yas}



\newcounter{subListing}[subfigure]

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{mygreen}{RGB}{28,172,0} 
\definecolor{mylilas}{RGB}{170,55,241}
\definecolor{backcolour}{rgb}{1,1,0.98}
\definecolor{backcolour}{rgb}{0.98,0.98,0.98}

\lstset{language=Matlab,%
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{blue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\tt\scriptsize,
	frame = LBtr,
	%frameround=T,
	rulecolor=\color{gray},
	showstringspaces=false,
	numbers=left,%
	numberstyle={\tiny\color{gray}},
	numbersep=8pt,
	breaklines=true,
	%postbreak=\mbox{\textcolor{yellow}{$\hookrightarrow$}\space},
	tabsize=2,
	escapechar=`,
	xleftmargin=1.8 em, 
	framexleftmargin=2em,
}



\newcommand\minimize[1]{\mathop{\text{\lr{minimize}}}\limits_{#1}\ }


\linespread{1.5}


\begin{document}

	\begin{titlepage}
		\begin{center}
			
			\includegraphics[width=4cm]{assets/logo-sharif.png} % آرم دانشگاه صنعتی شریف
			
			\vspace{1cm}
			
			{\huge \textbf{دانشگاه صنعتی شریف}}  
			
			\vspace{0.5cm}
			
			{\Large دانشکده مهندسی برق}
			
			\vspace{1.5cm}
			
			{\Huge \textbf{گزارش پروژه پایانی}} 
			
			\vspace{0.5cm}
			
			{\Large درس بهینه‌سازی محدب ۲}
			
			\vspace{1.5cm}
			
			{\Huge \textbf{یادگیری گراف با استفاده از همواری}}
			
			\vspace{1.5cm}
			
			\textbf{\Large استاد درس:}  
			
			\vspace{0.5cm}
			
			{\Large دکتر حامد شاه‌منصوری}
			
			\vspace{1.5cm}
			
			\textbf{\Large تهیه‌کنندگان:}  
			
			\vspace{0.5cm}
			
			{\Large محمد رضیئی فیجانی، علی مجلسی}
			
			\vfill
			
			{\Large \today}
			
		\end{center}
	\end{titlepage}
	


    \section{مقدمه}
    در حوزه‌هایی که داده‌ها ساختار مشخصی ندارند، استفاده از گراف‌ها بسیار مفید است. برای مثال، سیگنال‌های زمانی دارای ساختاری هستند که ترتیب داده‌ها توسط بُعد زمان تعیین می‌شود، یا تصاویر که سیگنال‌های مکانی با ترتیب و مجاورت پیکسل‌ها مشخص می‌شوند. اما داده‌هایی نیز وجود دارند که ساختار مشخصی ندارند، مانند داده‌های سنسورهایی که دمای نقاط مختلف یک شهر را اندازه‌گیری می‌کنند و به‌صورت نامنظم در سطح شهر پراکنده‌اند، یا داده‌هایی با ساختار گراف‌مانند، مانند تعاملات کاربران در شبکه‌های اجتماعی نظیر اینستاگرام، جایی که سیگنال علاقه‌مندی هر کاربر می‌تواند در سیستم‌های توصیه‌گر استفاده شود.
    
    به همین دلیل، حوزه‌های جدیدی در پردازش سیگنال و شبکه‌های عصبی به نام 
	\textit{پردازش سیگنال گرافی}\LTRfootnote{Graph Signal Processing}(\lr{GSP})\cite{GSP}
     و
	\textit{شبکه‌های عصبی گراف}\LTRfootnote{Graph Neural Networks}(\lr{GNN})\cite{GNN}
      به وجود آمده‌اند.
      برای استفاده از روش‌های پردازش سیگنال روی گراف و شبکه‌های عصبی گراف ، ابتدا نیاز به یک ساختار گراف داریم که داده‌ها روی آن تعریف شوند. در بسیاری از موارد، داده‌های ما به‌صورت طبیعی دارای ساختار گرافی نیستند و باید این ساختار را از روی داده‌ها استخراج کنیم؛ فرآیندی که به آن 
            \textit{یادگیری گراف}\LTRfootnote{Graph Learning}
      گفته می‌شود.
      
     
     \subsection{صورت مسئله}
     
     
     
     در یادگیری گراف، یکی از معیارهای کلیدی برای یافتن ساختار مناسب، هموار بودن داده‌ها روی گراف است. این معیار بیان می‌کند که در یک گراف بهینه، گره‌های متصل باید مقادیر مشابهی داشته باشند؛ به عبارت دیگر، داده‌ها نباید تغییرات ناگهانی در یال‌های گراف نشان دهند. برای مدل‌سازی این خاصیت، از انرژی دیریشله استفاده می‌شود که نشان‌دهنده‌ی میزان تغییرات داده‌ها روی گراف است. این انرژی بر اساس عملگر لاپلاسین گراف تعریف می‌شود:
     \begin{equation}
     	     \|\nabla_G \bm{X}\|_F^2 = \text{tr} (\bm{X}^\top \bm{L} \bm{X})
     \end{equation}
     
     در این رابطه، \( \nabla_G \bm{X} \) گرادیان گرافی داده‌های \( \bm{X} \) را نشان می‌دهد و \( \bm{L} \) ماتریس لاپلاسین گراف است که از رابطه‌ی زیر محاسبه می‌شود:
     
\begin{equation}\label{eq:defL}
	     \bm{L} = \bm{D} - \bm{W}
\end{equation}
     
     که در آن \( \bm{D} \) ماتریس درجه است که مقدار مجموع وزن‌های یال‌های هر گره را در قطر خود دارد و \( \bm{W} \) ماتریس وزن‌های گراف است که ارتباط بین گره‌ها را نشان می‌دهد.
     \begin{equation}\label{eq:defD}
     	\bm{D} = \operatorname{diag}(\bm{W}\mathds{1})
     \end{equation}
     
      انرژی دیریشله را می‌توان به‌صورت مجموع وزن‌دار اختلاف بین مقادیر گره‌ها نیز نوشت:
  \begin{equation}
  	     \frac{1}{2} \sum_{i,j} w_{ij} \|\bm{x}_i - \bm{x}_j\|_2^2
  \end{equation}
     
     این رابطه بیان می‌کند که مجموع اختلافات بین مقادیر گره‌های متصل، به وزن یال‌ها بستگی دارد. مقدار \( w_{ij} \) هرچه بیشتر باشد، یعنی ارتباط بین دو گره قوی‌تر است و در نتیجه اختلاف بین مقدار داده‌های آن دو گره باید کمتر باشد. برای نمایش دقیق‌تر این ارتباط، می‌توان از تعریف فاصله \( z_{ij} \) استفاده کرد:
\begin{equation}
	     z_{ij} = \|\bm{x}_i - \bm{x}_j\|^2
\end{equation}
     
     در اینجا \( \bm{Z} \) ماتریس فاصله بین داده‌های متناظر با گره‌ها است. به این ترتیب، تابع بهینه‌سازی به‌صورت فشرده‌تر نوشته می‌شود:
\begin{equation}
	     \frac{1}{2} \| \bm{W} \circ \bm{Z} \|_{1,1}
\end{equation}
     
     که در آن \( \circ \) عملگر ضرب هادامارد (عنصری) بین دو ماتریس است. این تابع نشان می‌دهد که یادگیری گراف بر مبنای هموار بودن داده‌ها، به یافتن ماتریس وزن‌های \( \bm{W} \) بستگی دارد که هموارترین تغییرات را در داده‌ها تضمین کند.
     
     برای یادگیری گراف با استفاده از معیار همواری، رویکردهای مختلفی پیشنهاد شده‌اند که هر کدام قیود متفاوتی بر روی ماتریس وزن‌های \( \bm{W} \) یا لاپلاسین \( \bm{L} \) اعمال می‌کنند. یکی از ساده‌ترین روش‌ها برای یادگیری گراف، کمینه‌سازی انرژی دیریشله \( \text{tr}(\|\bm{L} \bm{X}\|^2_F) \) تحت قیودی بر روی ماتریس وزن‌ها است.
     در روش \cite{Daitch2009}، این قیود شامل محدودیت \( \bm{W} \mathds{1} \geq \mathds{1} \) است که اطمینان می‌دهد هر گره حداقل مقدار مشخصی از اتصال را حفظ کند. 
     \begin{equation}
     		\begin{array}{cl}
     			\minimize{\bm{W}\in\mathcal{W}_m} & \text{tr}(\|\bm{L} \bm{X}\|^2_F) \\
     			\text{\lr{subject to}} & \bm{W} \mathds{1} \ge \mathds{1} \\
     			& \eqref{eq:defD}, \eqref{eq:defL}
     		\end{array}
     \end{equation}
     
     همچنین در نسخه دیگری از این روش، محدودیت \( \mathds{1}^\top \max(0, \bm{W} \mathds{1})^2 \leq \alpha n \) اعمال شده که میزان یال‌های اضافه را کنترل می‌کند. 
      \begin{equation}
     	\begin{array}{cl}
     		\minimize{\bm{W}\in\mathcal{W}_m} & \text{tr}(\|\bm{L} \bm{X}\|^2_F) \\
     		\text{\lr{subject to}} & \mathds{1}^\top \max(0, \bm{W} \mathds{1})^2 \leq \alpha n  \\
     		& \eqref{eq:defD}, \eqref{eq:defL}
     	\end{array}
     \end{equation}
     این قیود برای جلوگیری از یادگیری گراف‌های بیش‌ازحد متراکم و نامناسب طراحی شده‌اند. همچنین 
     $\mathcal{W}_m$
     مجموعه ماتریس های متقارن را نشان می‌دهد یعنی:
     \begin{equation}
     	  \mathcal{W}_m = \big\{\bm{W} | \bm{W} = \bm{W}^\top, \bm{W} \in \mathds{R}^{n\times n}\big\}
     \end{equation}
     
     مدل \cite{Lake2010DiscoveringSB} تابع هدف را گسترش داده و از یک ترم لگاریتمی روی دترمینان لاپلاسین به همراه یک منظم‌کننده‌ی \( \ell_1 \) روی ماتریس وزن‌ها استفاده می‌کند:
     \begin{equation}
     	     \minimize{\bm{W}} \text{tr} (\bm{X}^\top \bm{L} \bm{X}) - \log | \bm{L} + \alpha \bm{I} | + \beta \| \bm{W} \|_{1,1}
     \end{equation}
     
     ترم \( \log | \bm{L} + \alpha \bm{I} | \) از تکین شدن \( \bm{L} \) جلوگیری می‌کند و منظم‌کننده \( \ell_1 \) باعث می‌شود که بسیاری از درایه‌های \( \bm{W} \) صفر شوند، که منجر به یادگیری یک گراف تنک می‌شود.
     
     در روش‌های
     \cite{Hu2015,Dong2016}
      علاوه بر انرژی دیریشله، یک ترم اضافی نرم فروبنیوس برای کنترل مقدار \( \bm{L} \) اضافه شده است:
     
     \begin{equation}
     	     \minimize{\bm{W}} \text{tr} (\bm{X}^\top \bm{L} \bm{X}) + \alpha \|\bm{L}\|_F^2 \quad \text{s.t.} \quad \text{tr}(\bm{L}) = n
     \end{equation}
     
     وجود \( \|\bm{L}\|_F^2 \) در تابع هدف به جلوگیری از مقدارهای بیش‌ازحد بزرگ در ماتریس لاپلاسین کمک می‌کند و شرط \( \text{tr}(\bm{L}) = n \) مقیاس گراف را کنترل می‌کند تا بیش از حد کوچک یا پراکنده نشود.
     
     مدل 
     \cite{Kalofolias2016}
     با اضافه کردن یک ترم لگاریتمی روی مجموع وزن‌ها و یک منظم‌کننده‌ی نرم فروبنیوس برای کنترل پیچیدگی گراف، یک روش جامع‌تر برای یادگیری گراف ارائه می‌دهد:
     
     \[
     \min \text{tr} (\bm{X}^\top \bm{L} \bm{X}) - \alpha \mathds{1}^\top \log (\bm{W} \mathds{1}) + \frac{\beta}{2} \|\bm{W}\|_F^2
     \]
     
     ترم \( \mathds{1}^\top \log (\bm{W} \mathds{1}) \) باعث می‌شود که وزن‌های گراف به شکلی کنترل‌شده توزیع شوند و از تشکیل یال‌های بیش از حد ضعیف یا بسیار قوی جلوگیری شود، در حالی که ترم \( \|\bm{W}\|_F^2 \) از مقدارهای بیش‌ازحد بزرگ در وزن‌های یال‌ها جلوگیری می‌کند. این روش یک رویکرد بهینه برای یادگیری گراف‌های هموار ارائه می‌دهد که می‌تواند در کاربردهای مختلفی از جمله تحلیل شبکه‌های اجتماعی، پردازش سیگنال‌های گرافی و مدل‌سازی داده‌های غیرساختاریافته مورد استفاده قرار گیرد.
     
     هر یک از این روش‌ها، به طور خاص در تنظیم مقادیر یال‌ها و بهینه‌سازی لاپلاسین گراف مؤثر هستند و بسته به ویژگی‌های داده و نیازمندی‌های خاص یک مسئله، انتخاب مناسبی از میان این مدل‌ها می‌تواند صورت گیرد. مدل‌هایی که شامل قیود حداقلی برای درجه گره‌ها هستند، برای کاربردهایی مناسب‌اند که در آن‌ها اتصال حداقلی بین گره‌ها ضروری است. از سوی دیگر، روش‌هایی که از منظم‌کننده‌های \(\ell_1\) یا \(\ell_2\) استفاده می‌کنند، در یادگیری گراف‌های تنک کارایی بالاتری دارند، زیرا از ایجاد ارتباطات غیرضروری بین گره‌ها جلوگیری می‌کنند. همچنین، مدل‌هایی که از تابع لگاریتمی روی دترمینان لاپلاسین بهره می‌برند، تضمین می‌کنند که ساختار گراف دچار مشکلات عددی ناشی از تکین شدن ماتریس لاپلاسین نشود.
     
     در نهایت، تعادل میان پیچیدگی ساختاری گراف و میزان همواری داده‌ها، به عنوان یک مسئله‌ی بهینه‌سازی در یادگیری گراف مطرح می‌شود. مدل‌های مختلف ارائه‌شده نشان می‌دهند که می‌توان با انتخاب مناسب تابع هدف و قیود مربوطه، ساختار گرافی مطلوبی را یاد گرفت که ویژگی‌های ذاتی داده‌ها را به خوبی بازتاب دهد. این حوزه از تحقیق همچنان در حال پیشرفت است و روش‌های جدیدتر با در نظر گرفتن قیود فیزیکی، توپولوژیکی و داده‌محور، در حال توسعه هستند تا فرآیند یادگیری گراف را بهینه‌تر و کاربردی‌تر کنند.
     
     
     
     
     
     
     
      
    \section{صورت مسئله}
    
	\section{مقدمه}
	در این گزارش، فرآیند یادگیری گراف با استفاده از روش \cite{Kalofolias2016} مورد بررسی قرار می‌گیرد. هدف این روش، یادگیری یک گراف از سیگنال‌های صاف (smooth signals) است به‌طوری‌که ساختار گراف به‌طور بهینه با داده‌ها سازگار باشد.
	
	\section{فرآیند یادگیری گراف}
	
	\subsection{گراف اولیه}
	ابتدا یک گراف اولیه به‌صورت شبکه‌ی دوبعدی با نویز تصادفی ایجاد می‌شود. مختصات گره‌ها شامل مقداری نویز است تا تنوع داده‌ها افزایش یابد.
	
	\begin{latin}
		\lstinputlisting[firstline=4,lastline=10]{codes/simple.m}
	\end{latin}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{codes/results/graph-initial.pdf}
		\caption{ساختار گراف اولیه.}
		\label{fig:graph-initial}
	\end{figure}
	
	\subsection{ماتریس فواصل جفتی}
	برای یادگیری گراف، ابتدا ماتریس فواصل جفتی \( Z \) محاسبه می‌شود. این ماتریس به‌صورت زیر تعریف می‌شود:
	
	\[
	Z_{ij} = \|\mathbf{x}_i - \mathbf{x}_j\|^2
	\]
	
	که در آن \( \mathbf{x}_i \) و \( \mathbf{x}_j \) مختصات گره‌های \( i \) و \( j \) هستند.
	
	\begin{latin}
		\lstinputlisting[firstline=13,lastline=17]{codes/simple.m}
	\end{latin}
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{codes/results/pairwise-distances.pdf}
		\caption{ماتریس فواصل جفتی \( Z \).}
		\label{fig:pairwise-distances}
	\end{figure}
	
	\subsection{یادگیری گراف با پارامترهای ثابت}
	در این مرحله، با استفاده از روش \cite{Kalofolias2016} و با تنظیم پارامترهای \( a = 1 \) و \( b = 1 \)، ماتریس وزن‌های گراف \( W \) یاد گرفته می‌شود. تابع هدف به‌صورت زیر است:
	
	\[
	\min_{\bm{W}} \sum_{i,j} z_{ij} w_{ij} + a \sum_{i,j} {w}_{ij} \log {w}_{ij} + b \|\bm{W}\|_F^2
	\]
	
	که در آن
	 \( \|\bm{W}\|_F \)
	 نرم فروبنیوس ماتریس \( W \) است.
	
	\begin{latin}
		\lstinputlisting[firstline=20,lastline=26]{codes/simple.m}
	\end{latin}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{codes/results/graph-learned-ab1.pdf}
		\caption{گراف یادگرفته‌شده با \( a = 1 \) و \( b = 1 \).}
		\label{fig:graph-learned-ab1}
	\end{figure}
	
	\subsection{یادگیری گراف با تعداد یال‌های متوسط مشخص}
	برای کنترل میزان پراکندگی گراف، می‌توان پارامتر \( \theta \) را به‌گونه‌ای تنظیم کرد که هر گره به‌طور متوسط دارای \( k \) یال باشد. مقدار \( \theta \) با استفاده از رابطه‌ی زیر محاسبه می‌شود:
	
	\[
	\theta = \frac{k}{\sum_{i,j} Z_{ij}}
	\]
	
	سپس با استفاده از این مقدار، ماتریس وزن‌های گراف \( W \) یاد گرفته می‌شود.
	
	\begin{latin}
		\lstinputlisting[firstline=29,lastline=44]{codes/simple.m}
	\end{latin}
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{codes/results/graph-learned-k4.pdf}
		\caption{گراف یادگرفته‌شده با \( k = 4 \) یال در هر گره به‌طور متوسط.}
		\label{fig:graph-learned-k4}
	\end{figure}
	
	\subsection{نمایش نهایی گراف}
	در نهایت، ساختار گراف یادگرفته‌شده به‌صورت بهینه و با کاهش اتصالات ضعیف نمایش داده می‌شود.
	
	\begin{latin}
		\lstinputlisting[firstline=47,lastline=49]{codes/simple.m}
	\end{latin}
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{codes/results/graph-final.pdf}
		\caption{ساختار نهایی گراف.}
		\label{fig:graph-final}
	\end{figure}
	
    	

    
    \bibliographystyle{plainnat-fa}
    \bibliography{assets/references}
	\nocite{*}
\end{document}